\documentclass[aspectratio=169]{beamer} 

\usepackage{hayesmacros}

\usetheme[block=fill]{metropolis}
\setsansfont{Fira Sans}  % be sure to compile with XeLaTeX
% \setmonofont{Fira Mono}  % be sure to compile with XeLaTeX

\usepackage{natbib}
\usepackage{nicematrix}
\usepackage{fontawesome5}
\usepackage{xcolor}

\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\setbeamercolor{background canvas}{bg=white}
\setbeamercolor{normal text}{fg=black}
\setbeamercolor{frametitle}{bg=black, fg=white}

\hypersetup{colorlinks,citecolor=cyan, urlcolor=cyan, linkcolor=black}


\title{Asymptotic unidentifiability of peer effects in the linear-in-means model}
\date{2024-04-04 @ 12:30 pm\\ Service Memorial Institute 133, Medical Sciences Center, UW-Madison}
\author{Alex Hayes | PhD Defense}
\institute{Department of Statistics, University of Wisconsin-Madison}

\begin{document}

\maketitle

% {
%     \usebackgroundtemplate{\includegraphics[width=\paperwidth]{figures/this_is_fine.jpeg}}
%     \begin{frame}
%     \end{frame}
% }

\begin{frame}{What are peer effects?}
    \begin{columns}
        \column{0.5\textwidth}
        \vspace{7mm}
        
        \underline{Contagion}: if my friends get sick, I am more likely to set sick
        
        \vspace{4mm}
        
        \underline{Direct effect}: if I get vaccinated, I am less likely to get sick
        
        \vspace{4mm}
        
        \underline{Interference}: if my friends get vaccinated, I am less likely to get sick
        
        \vspace{7mm}
        
        {\footnotesize * Not a causal talk. But causally inspired.}
        
        \column{0.5\textwidth}
        \begin{figure}[ht]
            \centering
            \includegraphics[width=\textwidth]{figures/assortative.png}
        \end{figure}
    \end{columns}
\end{frame}

\begin{frame}{The canonical linear model for peer effects}
    
    \begin{align*}
        \underbrace{Y_i}_\text{sick?} =
        \underbrace{\alpha}_{\substack{\text{base}                                    \\ \text{rate}}} +
        \underbrace{\beta}_{\substack{\text{contagion}                                \\ \text{effect}}}
        \underbrace{\sum_{i \neq j} \frac{A_{ij}}{d_i} Y_j}_{\substack{\text{portion} \\ \text{sick} \\ \text{peers}}} + 
        \underbrace{\gamma}_{\substack{\text{direct}                                  \\ \text{effect}}}
        \underbrace{T_i}_\text{vaccinated?} + 
        \underbrace{\delta}_{\substack{\text{interference}                            \\ \text{effect}}}
        \underbrace{\sum_{i \neq j} \frac{A_{ij}}{d_i} T_j}_{\substack{\text{portion} \\ \text{vaccinated} \\ \text{peers}}} +
        \underbrace{\varepsilon_i}_\text{error}
    \end{align*}
    
    {
    %\footnotesize
    \begin{table}[]
        \begin{tabular}{lcl}
            Outcome          & $Y_i$    & $\in \{0, 1\}$              \\
            Treatment        & $T_i$    & $\in \{0, 1\}$              \\
            Adjacency matrix & $A$      & $\in \{0, 1\}^{n \times n}$ \\
            Edge $i \sim j$  & $A_{ij}$ & $\in \{0, 1\}$              \\
            Node degree      & $d_i$    & $\in \mathbb{Z}^+$          \\
        \end{tabular}
    \end{table}
    }
\end{frame}

\begin{frame}
    \vspace{5mm}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/manski1993.png}
    \end{figure}
\end{frame}

\begin{frame}{Manski's reflection problem: highly structured networks break the model}
    
    \tikzset{every loop/.style={}}
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \begin{tikzpicture}
                    \node[shape=circle,draw=black,label=above left:{$T_A = 1$}] (A) at (0,1) {A};
                    \node[shape=circle,draw=black,label=below left:{$T_B = 0$}] (B) at (1,0) {B};
                    \node[shape=circle,draw=black,label=above right:{$T_C = 1$}] (C) at (1.5,1.5) {C};
                    \node[shape=circle,draw=black,label=above right:{$T_D = 1$}] (D) at (2.75,0.5) {D};
                    
                    \path (A) edge [loop above] node {} (A);
                    \path (B) edge [loop below] node {} (B);
                    \path (C) edge [loop above] node {} (C);
                    \path (D) edge [loop above] node {} (D);
                    
                    \draw (A) -- (B);
                    \draw (A) -- (C);
                    \draw (A) -- (D);
                    \draw (B) -- (C);
                    \draw (B) -- (D);
                    \draw (C) -- (D);
                \end{tikzpicture}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            
            Average value of $T$ amongst peers is the same for all nodes!
            
            \begin{align*}
                GT_A & = 3/4  \\
                GT_B & = 3/4  \\
                GT_C & = 3/4  \\
                GT_D & = 3/4
            \end{align*}
            
            \underline{Problem:} cannot distinguish base rate $\alpha$ from interference effect $\delta$
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Manski's reflection problem: highly structured networks break the model}
    
    \tikzset{every loop/.style={}}
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \begin{tikzpicture}
                    \node[shape=circle,draw=black,label=above left:{$T_A = 1$}] (A) at (0,1) {A};
                    \node[shape=circle,draw=black,label=below left:{$T_B = 0$}] (B) at (1,0) {B};
                    \node[shape=circle,draw=black,label=above right:{$T_C = 1$}] (C) at (1.5,1.5) {C};
                    \node[shape=circle,draw=black,label=above right:{$T_D = 1$}] (D) at (2.75,0.5) {D};
                    
                    \path (A) edge [loop above] node {} (A);
                    \path[] (B) edge [loop below] node {} (B);
                    \path (C) edge [loop above] node {} (C);
                    \path (D) edge [loop above] node {} (D);
                    
                    \draw (A) -- (B);
                    \draw (A) -- (C);
                    \draw (A) -- (D);
                    \draw (B) -- (C);
                    \draw (B) -- (D);
                    \draw (C) -- (D);
                \end{tikzpicture}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{equation*}
                \begin{bmatrix}
                    Y_A \\
                    Y_B \\
                    Y_C \\
                    Y_D
                \end{bmatrix}
                =
                \underbrace{
                    \begin{bNiceMatrix}[first-row,first-col]
                         & 1_n & GY   & T & GT  \\
                         & 1   & GY_A & 1 & 3/4 \\
                         & 1   & GY_B & 0 & 3/4 \\
                         & 1   & GY_C & 1 & 3/4 \\
                         & 1   & GY_D & 1 & 3/4 \\
                    \end{bNiceMatrix}
                }_W
                \begin{bmatrix}
                    \alpha \\
                    \beta  \\
                    \gamma \\
                    \delta
                \end{bmatrix}
                +
                \begin{bmatrix}
                    \varepsilon_A \\
                    \varepsilon_B \\
                    \varepsilon_C \\
                    \varepsilon_D
                \end{bmatrix}
            \end{equation*}
            
            \underline{Problem}: Design matrix $W$ becomes collinear!
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Identification}
    
    Define the degree matrix $D = \diag(d_1, d_2, \dots, d_n)$, where $d_i = \sum_j A_{ij}$. Let $G = D^{-1} A$ be the row-normalized adjacency matrix. Then
    \begin{equation*} \label{eq:lim-mv}
        Y = \alpha 1_n + \beta G Y + T \gamma + G T \delta + \varepsilon.
    \end{equation*}
    
    \begin{definition}
        We say that $(\alpha, \beta, \gamma, \delta)$ are \underline{identified} when the columns of the design matrix 
        \begin{equation*}
            \label{eq:design}
            W_n = \begin{bmatrix} 1_n & GY & T & GT \end{bmatrix}.
        \end{equation*}
        are linearly independent. Otherwise, we say that $(\alpha, \beta, \gamma, \delta)$ are \underline{unidentified}.
    \end{definition}
    
    We assume that $\mathbb E \left[\varepsilon | W_n \right] = 0$.
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{figures/bramoulle2009.png}
    \end{figure}
\end{frame}

\begin{frame}{Bramoull\'e: intransivity (i.e, open triangles) fixes the problem}
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}
                \centering
                \begin{tikzpicture}
                    \node[shape=circle,draw=black] (A) at (0,1) {A};
                    \node[shape=circle,draw=black] (B) at (1,0) {B};
                    \node[shape=circle,draw=black] (C) at (1.5,1.5) {C};
                    \node[shape=circle,draw=black] (D) at (2.75,0.5) {D};
                    
                    \draw (A) -- (B);
                    \draw (A) -- (C);
                    \draw (A) -- (D);
                    \draw (C) -- (D);
                \end{tikzpicture}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            
            $A \leftrightarrow B \leftrightarrow D$ is an intransitive triangle. If $B$ were friends with $D$ it would ``close'' the triangle
            
            \vspace{4mm}
            
            \underline{Since 2009:} as long as there is intransitivity in the network, $(\alpha, \beta, \gamma, \delta)$ are identified, practitioners are good to use the linear-in-means model
            
            \vspace{4mm}
            
            Reflection problem solved!
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \begin{proposition}[Finite sample identification, \cite{bramoulle2009}]
        Let $\varepsilon$ be mean zero, i.i.d. noise and let
        \begin{align*}
            Y & = \alpha 1_n + \beta G Y + \gamma T + \delta G T + \varepsilon                        
        \end{align*}
        Suppose that $\abs{\beta} < 1$ and $\gamma \beta + \delta \neq 0$. If $I, G$ and $G^2$ are linearly independent, in the sense that $a I + b G + c G^2 = 0$ requires $a = b = c = 0$, then $\alpha, \beta, \gamma$ and $\delta$ are identified.
    \end{proposition}
    
    $\gamma \beta + \delta \neq 0$ means that there is either some interference effect, or some direct effect and some contagion effect, and if there are both, they don't cancel each other out.
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics{figures/simulations/defense-mse.pdf}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics{figures/simulations/defense-backbone.pdf}
    \end{figure}
\end{frame}


\begin{frame}{The problem: interference effect is an average of i.i.d. random variables}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            Even though the network is intransitive, heading towards a situation like
            
            \begin{equation*}
                \begin{bmatrix}
                    Y_1    \\
                    Y_2    \\
                    Y_3    \\
                    Y_4    \\
                    \vdots \\
                    Y_n
                \end{bmatrix}
                =
                \begin{bNiceMatrix}[first-row,first-col]
                     & 1_n    & GY     & T      & GT       \\
                     & 1      & GY_1   & 1      & 1/2      \\
                     & 1      & GY_2   & 0      & 1/2      \\
                     & 1      & GY_3   & 1      & 1/2      \\
                     & 1      & GY_4   & 0      & 1/2      \\
                     & \vdots & \vdots & \vdots & \vdots   \\
                     & 1      & GY_n   & 1      & 1/2    
                \end{bNiceMatrix}
                \begin{bmatrix}
                    \alpha \\
                    \beta  \\
                    \gamma \\
                    \delta
                \end{bmatrix}
                +
                \begin{bmatrix}
                    \varepsilon_1 \\
                    \varepsilon_2 \\
                    \varepsilon_3 \\
                    \varepsilon_4 \\
                    \vdots        \\
                    \varepsilon_n
                \end{bmatrix}
            \end{equation*}
        \end{column}
        \begin{column}{0.4\textwidth}
            The $GT$ term converges to a constant
            
            \begin{align*}
                \sum_{i \neq j} \frac{A_{ij}}{d_i} T_j \to \frac 12                       
            \end{align*}
            
            as $d_i \to \infty$
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}{$GY$ term also becomes colinear with intercept}
    Recall
    \begin{align*}
        Y & = \alpha 1_n + \beta G Y + \gamma T + \delta G T + \varepsilon
    \end{align*}
    If $\abs{\beta} < 1$, then $I - \beta G$ is invertible and there is a unique solution 
    \begin{align*}
        Y & = \paren*{I - \beta G}^{-1} \paren*{\alpha 1_n + \gamma T + \delta G T + \varepsilon}                                                                                \\
          & = \sum_{k=0}^\infty \beta^k G^k \paren*{\alpha 1_n + \gamma T + \delta G T + \varepsilon} & \text{since } \paren*{I - \beta G}^{-1} = \sum_{k=0}^\infty \beta^k G^k
    \end{align*}
    
    * This ``reduced form'' of the linear-in-means model also characterizes the data generating process
\end{frame}

\begin{frame}{$GY$ term also becomes colinear with intercept}
    Suppose no node is isolated. Then, the reduced form of $Y$ is given by
    \begin{align*}
        Y & = \frac{\alpha}{1 - \beta} 1_n  + \gamma T + (\gamma \beta + \delta) \sum_{k=0}^\infty \beta^k G^{k+1} T + \sum_{k=0}^\infty \beta^k G^k \varepsilon
    \end{align*}
    and further
    \begin{align*}
        GY = 
        \frac{\alpha}{1 - \beta} 1_n + 
        \underbrace{\gamma G T}_{\substack{\text{neighborhood}                                              \\ \text{average}}} + 
        \underbrace{(\gamma \beta + \delta) \sum_{k=0}^\infty \beta^k G^{k+2} T}_{\substack{\text{repeated} \\ \text{averages} \\ \text{of $T$}}} +
        \underbrace{\sum_{k=0}^\infty \beta^k G^{k+1} \varepsilon}_{\substack{\text{repeated}               \\ \text{averages} \\ \text{of $\varepsilon$}}}
    \end{align*}
    If neighborhood averages $GT$ are converging, repeated neighborhood averages converge as well, because $G 1_n = 1_n$ (neighborhood average of constants is constant)
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics{figures/simulations/defense-vif.pdf}
    \end{figure}
\end{frame}

\begin{frame}

    \begin{definition}
        We say that $(\alpha, \beta, \gamma, \delta)$ are \underline{asymptotically identified} when the design matrix \eqref{eq:design} converges to a limit object $W$ in the sense that
        \begin{equation*}
            \max_{ij} \abs*{
            \begin{bmatrix} 1_n & GY & T & GT \end{bmatrix}_{ij} - W_{ij}
            } = o(1)
        \end{equation*}
        and the columns of the $W$ are linearly independent. If the columns of $W$ are linearly dependent, we say that $(\alpha, \beta, \gamma, \delta)$ are asymptotically unidentified.
    \end{definition}
\end{frame}

\begin{frame}

    \begin{assumption}[1]
        \label{ass:nonid}
        Suppose that
        \begin{enumerate}
            \item $T_1,T_2,\dots,T_n$ are independent with shared mean $\zeta \in \R$, and $T$ is independent of $A$.
            \item $\{ T_i - \zeta : i \in [n] \}$ are independent $(\nu,b)$-subgamma random variables.
            \item $\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n$ are independent subgamma random variables with parameters not depending on $n$.
                  % more precisely: the sequence of networks A(n) is such that
            \item $A$ is such that
                  \begin{equation*}
                      \max_{i \in [n]} \frac{1}{d_i^2} \sum_{j=1}^n A_{ij}^2
                      = o\left( \frac{ 1 }{ \nu \log^2 n } \right)
                      ~\text{and}~
                      \max_{j \in [n]} \frac{ A_{ij} }{ d_i }
                      = o\left( \frac{ 1 }{ b \log n } \right).
                  \end{equation*}
        \end{enumerate}
    \end{assumption}
    
    
    The fourth condition requires that the size of each neighborhood is growing. When the network is binary, such that $A_{ij} \in \set{0, 1}$, it reduces to $\min_{i \in [n]} d_i = \omega(\log n)$.
    That is, the size of the smallest neighborhood must grow at a slightly faster than $\log n$ rate. This implies that no nodes in the network are isolated. In the more general case of a weighted, non-binary network, the third condition it requires that no individual edge accounts for too much of the total ``weight'' incident on any vertex.
\end{frame}

\begin{frame}

    \begin{lemma}[1] \label{thm:indepcov:nonid}
        Under Assumption~\ref{ass:nonid},
        \begin{equation*}
            \max_{i \in [n]} \Big\| [GT]_i - \zeta \Big\|
            = o(1) ~ \text{ almost surely }
        \end{equation*}
        and there exists $\eta = \eta(\zeta, \alpha, \beta, \gamma, \delta) \in \R$ such that
        \begin{equation*}
            \max_{i \in [n]} \Big\| [GY]_i - \eta \Big\|
            = o(1) ~ \text{ almost surely.}
        \end{equation*}
    \end{lemma}
    
    \begin{theorem}[1]
        \label{cor:indep}
        Under Assumption \ref{ass:nonid}, $\alpha, \beta$ and $\delta$ are asymptotically unidentified.
    \end{theorem}
\end{frame}

\begin{frame}{Caveats}
    \begin{enumerate}
        \item \underline{Isolated nodes}: If all the connected components that are not singletons satisfying Assumption \ref{ass:nonid}, can recover $\alpha$ but $\beta$ and $\delta$ are still aliased.
        \item \underline{Sparse networks}: Our results don't cover sparse networks!
        \item \underline{Non-random covariates}: Our results don't cover fixed $T$!
        \item \underline{Independence}: What if $T$ and the network $A$ are dependent?
    \end{enumerate}
\end{frame}

\begin{frame}
    \begin{figure}
        \centering
        \includegraphics{figures/simulations/defense-backbone-dependent.pdf}
    \end{figure}
\end{frame}

\begin{frame}{Why dependence might help}
    The $GT$ term is \underline{not longer an average of independent observations}
    
    \begin{align*}
        \sum_{i \neq j} \frac{A_{ij}}{d_i} T_j                 
    \end{align*}
    
    so $GT$ might not converge to $\mathbb E[T]$ for every node.
\end{frame}

\begin{frame}
    \begin{theorem}[Identification failure in random dot product graphs]
        Let $\varepsilon$ be mean zero, i.i.d. noise and let
        \begin{align*}
            Y & = \alpha 1_n + \beta G Y + \gamma T + \delta G T + \varepsilon                        
        \end{align*}
        Suppose $\abs{\beta} < 1$. Let $(A, \X) \sim \text{RDPG}(F, n)$ and let $T = \X \in \R^{n \times d}$. Then there exists constants $C, C' \in \R^d$ such that
        \begin{align*}
            \lim_{n \to \infty} \sup_{i \in [n]} \abs*{GT - 1_n C}  & = \op{1},  & \text{and} \\
            \lim_{n \to \infty} \sup_{i \in [n]} \abs*{GY - 1_n C'} & = \op{1}.
        \end{align*}
    \end{theorem}
\end{frame}

\begin{frame}{Why this matters}
    
    \begin{enumerate}
        \item \cite{shalizi2011,mcfowland2021}: homophily and contagion are non-parametrically confounded. Must make parametric assumptions to estimate contagion effects.
        \item \cite{bramoulle2020}: most parametric contagion identification theory considers fixed $n$ setting but not the $n \to \infty$ limit. Many results won't hold under stochastic blockmodels.
    \end{enumerate}
\end{frame}

\begin{frame}{Open questions}
    
    \begin{itemize}
        \item Do $GT$ and $GY$ ever not converge to constants?
        \item Are longitudinal contagion models also affected?
        \item What happens in sparse networks asymptotically?
    \end{itemize}
\end{frame}

\begin{frame}{Thank you! Questions?}
    
    \underline{Stay in touch}
    
    \begin{itemize}
        \item[] \faIcon{twitter} \href{https://twitter.com/alexpghayes}{@alexpghayes}
        \item[] \faIcon[regular]{envelope} \href{mailto:alex.hayes@wisc.edu}{alex.hayes@wisc.edu}
        \item[] \faIcon{wordpress} \url{https://www.alexpghayes.com}
        \item[] \faIcon{github} \url{https://github.com/alexpghayes}
    \end{itemize}
    
\end{frame}

\appendix

\bibliographystyle{chicago}
\bibliography{references}

\end{document}